{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport fasttext\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:46.473272Z","iopub.execute_input":"2023-05-09T04:32:46.473680Z","iopub.status.idle":"2023-05-09T04:32:46.627023Z","shell.execute_reply.started":"2023-05-09T04:32:46.473644Z","shell.execute_reply":"2023-05-09T04:32:46.625761Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# @title Figure Settings\nimport ipywidgets as widgets\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:46.629763Z","iopub.execute_input":"2023-05-09T04:32:46.630674Z","iopub.status.idle":"2023-05-09T04:32:47.035981Z","shell.execute_reply.started":"2023-05-09T04:32:46.630612Z","shell.execute_reply":"2023-05-09T04:32:47.034521Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# @title Helper functions\ndef cosine_similarity(vec_a, vec_b):\n  \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n  return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n\n\ndef getSimilarity(word1, word2):\n  v1 = ft_en_vectors.get_word_vector(word1)\n  v2 = ft_en_vectors.get_word_vector(word2)\n  return cosine_similarity(v1, v2)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:47.037167Z","iopub.execute_input":"2023-05-09T04:32:47.037567Z","iopub.status.idle":"2023-05-09T04:32:47.045730Z","shell.execute_reply.started":"2023-05-09T04:32:47.037528Z","shell.execute_reply":"2023-05-09T04:32:47.044212Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# For DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness.\n  NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  \"\"\"\n  DataLoader will reseed workers following randomness in\n  multi-process data loading algorithm.\n\n  Args:\n    worker_id: integer\n      ID of subprocess to seed. 0 means that\n      the data will be loaded in the main process\n      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n\n  Returns:\n    Nothing\n  \"\"\"\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:47.049003Z","iopub.execute_input":"2023-05-09T04:32:47.050070Z","iopub.status.idle":"2023-05-09T04:32:49.522911Z","shell.execute_reply.started":"2023-05-09T04:32:47.050010Z","shell.execute_reply":"2023-05-09T04:32:49.521839Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# @title Set device (GPU or CPU). Execute `set_device()`\n\n# Inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  \"\"\"\n  Set the device. CUDA if available, CPU otherwise\n\n  Args:\n    None\n\n  Returns:\n    Nothing\n  \"\"\"\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.524683Z","iopub.execute_input":"2023-05-09T04:32:49.525960Z","iopub.status.idle":"2023-05-09T04:32:49.531951Z","shell.execute_reply.started":"2023-05-09T04:32:49.525917Z","shell.execute_reply":"2023-05-09T04:32:49.530835Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"DEVICE = set_device()\nSEED = 2021\nset_seed(seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.533359Z","iopub.execute_input":"2023-05-09T04:32:49.533923Z","iopub.status.idle":"2023-05-09T04:32:49.553658Z","shell.execute_reply.started":"2023-05-09T04:32:49.533885Z","shell.execute_reply":"2023-05-09T04:32:49.552319Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \nRandom seed 2021 has been set.\n","output_type":"stream"}]},{"cell_type":"code","source":"# import fasttext.util\n# # fasttext.util.download_model('en', if_exists='ignore')  # English\n# ft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.555456Z","iopub.execute_input":"2023-05-09T04:32:49.555931Z","iopub.status.idle":"2023-05-09T04:32:49.562276Z","shell.execute_reply.started":"2023-05-09T04:32:49.555879Z","shell.execute_reply":"2023-05-09T04:32:49.560899Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# import fasttext.util\n# # fasttext.util.download_model('es', if_exists='ignore')  # English\n# ft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.563934Z","iopub.execute_input":"2023-05-09T04:32:49.565261Z","iopub.status.idle":"2023-05-09T04:32:49.576395Z","shell.execute_reply.started":"2023-05-09T04:32:49.565204Z","shell.execute_reply":"2023-05-09T04:32:49.575397Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Lets see similarity without making bilingual embedding","metadata":{}},{"cell_type":"code","source":"# hello = ft_en_vectors.get_word_vector('hello')\n# hi = ft_en_vectors.get_word_vector('hi')\n# bonjour = ft_es_vectors.get_word_vector('bonjour')\n\n# print(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\n# print(f\"Cosine Similarity between BONJOUR and HELLO: {cosine_similarity(hello, bonjour)}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.577693Z","iopub.execute_input":"2023-05-09T04:32:49.578948Z","iopub.status.idle":"2023-05-09T04:32:49.589165Z","shell.execute_reply.started":"2023-05-09T04:32:49.578906Z","shell.execute_reply":"2023-05-09T04:32:49.588053Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# cat = ft_en_vectors.get_word_vector('cat')\n# chatte = ft_es_vectors.get_word_vector('chatte')\n# chat = ft_es_vectors.get_word_vector('chat')\n\n# print(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\n# print(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\n# print(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.593818Z","iopub.execute_input":"2023-05-09T04:32:49.594187Z","iopub.status.idle":"2023-05-09T04:32:49.601306Z","shell.execute_reply.started":"2023-05-09T04:32:49.594154Z","shell.execute_reply":"2023-05-09T04:32:49.600265Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# en_words = set(ft_en_vectors.words)\n# es_words = set(ft_es_vectors.words)\n# overlap = list(en_words & es_words)\n# bilingual_dictionary = [(entry, entry) for entry in overlap]","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.602759Z","iopub.execute_input":"2023-05-09T04:32:49.603800Z","iopub.status.idle":"2023-05-09T04:32:49.612595Z","shell.execute_reply.started":"2023-05-09T04:32:49.603745Z","shell.execute_reply":"2023-05-09T04:32:49.610828Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# print(bilingual_dictionary)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.614715Z","iopub.execute_input":"2023-05-09T04:32:49.615194Z","iopub.status.idle":"2023-05-09T04:32:49.624408Z","shell.execute_reply.started":"2023-05-09T04:32:49.615154Z","shell.execute_reply":"2023-05-09T04:32:49.623244Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# def make_training_matrices(source_dictionary, target_dictionary,\n#                            bilingual_dictionary):\n#   source_matrix = []\n#   target_matrix = []\n#   for (source, target) in tqdm(bilingual_dictionary):\n#     # if source in source_dictionary.words and target in target_dictionary.words:\n#     source_matrix.append(source_dictionary.get_word_vector(source))\n#     target_matrix.append(target_dictionary.get_word_vector(target))\n#   # return training matrices\n#   return np.array(source_matrix), np.array(target_matrix)\n\n\n# # from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n# def normalized(a, axis=-1, order=2):\n#   \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n#   l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n#   l2[l2==0] = 1\n#   return a / np.expand_dims(l2, axis)\n\n\n# def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n#   \"\"\"\n#   Source and target matrices are numpy arrays, shape\n#   (dictionary_length, embedding_dimension). These contain paired\n#   word vectors from the bilingual dictionary.\n#   \"\"\"\n#   # optionally normalize the training vectors\n#   if normalize_vectors:\n#     source_matrix = normalized(source_matrix)\n#     target_matrix = normalized(target_matrix)\n#   # perform the SVD\n#   product = np.matmul(source_matrix.transpose(), target_matrix)\n#   U, s, V = np.linalg.svd(product)\n#   # return orthogonal transformation which aligns source language to the target\n#   return np.matmul(U, V)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.625722Z","iopub.execute_input":"2023-05-09T04:32:49.626696Z","iopub.status.idle":"2023-05-09T04:32:49.637016Z","shell.execute_reply.started":"2023-05-09T04:32:49.626643Z","shell.execute_reply":"2023-05-09T04:32:49.635525Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# source_training_matrix, target_training_matrix = make_training_matrices(ft_en_vectors, ft_es_vectors, bilingual_dictionary)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.639103Z","iopub.execute_input":"2023-05-09T04:32:49.639594Z","iopub.status.idle":"2023-05-09T04:32:49.652478Z","shell.execute_reply.started":"2023-05-09T04:32:49.639552Z","shell.execute_reply":"2023-05-09T04:32:49.651206Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# transform = learn_transformation(source_training_matrix, target_training_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.654068Z","iopub.execute_input":"2023-05-09T04:32:49.654825Z","iopub.status.idle":"2023-05-09T04:32:49.663755Z","shell.execute_reply.started":"2023-05-09T04:32:49.654783Z","shell.execute_reply":"2023-05-09T04:32:49.662514Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Letâ€™s run the same examples as above, but this time, whenever we use French words, the matrix multiplies the embedding by the transpose of the transform matrix. That works a lot better!","metadata":{}},{"cell_type":"code","source":"# hello = ft_en_vectors.get_word_vector('hello')\n# hi = ft_en_vectors.get_word_vector('hi')\n# bonjour = np.matmul(ft_es_vectors.get_word_vector('bonjour'), transform.T)\n\n# print(f\"Cosine Similarity between HI and HELLO: {cosine_similarity(hello, hi)}\")\n# print(f\"Cosine Similarity between BONJOUR and HELLO: {cosine_similarity(hello, bonjour)}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.665477Z","iopub.execute_input":"2023-05-09T04:32:49.665927Z","iopub.status.idle":"2023-05-09T04:32:49.674991Z","shell.execute_reply.started":"2023-05-09T04:32:49.665885Z","shell.execute_reply":"2023-05-09T04:32:49.673972Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# cat = ft_en_vectors.get_word_vector('cat')\n# chatte = np.matmul(ft_es_vectors.get_word_vector('chatte'), transform.T)\n# chat = np.matmul(ft_es_vectors.get_word_vector('chat'), transform.T)\n\n# print(f\"Cosine Similarity between cat and chatte: {cosine_similarity(cat, chatte)}\")\n# print(f\"Cosine Similarity between cat and chat: {cosine_similarity(cat, chat)}\")\n# print(f\"Cosine Similarity between chatte and chat: {cosine_similarity(chatte, chat)}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.676625Z","iopub.execute_input":"2023-05-09T04:32:49.677093Z","iopub.status.idle":"2023-05-09T04:32:49.692743Z","shell.execute_reply.started":"2023-05-09T04:32:49.677055Z","shell.execute_reply":"2023-05-09T04:32:49.691524Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Embeddings have now being obtained","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom gensim.models import KeyedVectors\nimport nltk\n# nltk.download()\nimport nltk\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:49.694639Z","iopub.execute_input":"2023-05-09T04:32:49.695981Z","iopub.status.idle":"2023-05-09T04:32:51.181042Z","shell.execute_reply.started":"2023-05-09T04:32:49.695926Z","shell.execute_reply":"2023-05-09T04:32:51.179734Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# English\ntrain_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_train_df.csv')\nval_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_val_df.csv')\ntest_df_en = pd.read_csv('/kaggle/input/cleaned-data-eng/cleaned_test_df.csv')\n\n# Spanish\ntrain_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-train.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\nval_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-dev.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)\ntest_df_es = pd.read_csv('/kaggle/input/aaa1111/STS-train/data/stsb-multi-mt-main/data/stsb-es-test.csv', usecols=[0,1,2], names=['sent1', 'sent2', 'score'], header=None)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.182760Z","iopub.execute_input":"2023-05-09T04:32:51.183378Z","iopub.status.idle":"2023-05-09T04:32:51.375527Z","shell.execute_reply.started":"2023-05-09T04:32:51.183319Z","shell.execute_reply":"2023-05-09T04:32:51.374454Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nfrom sklearn.linear_model import LinearRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.378538Z","iopub.execute_input":"2023-05-09T04:32:51.379046Z","iopub.status.idle":"2023-05-09T04:32:51.386514Z","shell.execute_reply.started":"2023-05-09T04:32:51.378995Z","shell.execute_reply":"2023-05-09T04:32:51.384903Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Replace numbers with num\n    text = re.sub(r'\\d+', '', text)\n    # Lower case\n    text= text.lower()\n    sent_token = text.split()\n    # Lemmatize\n#     sent_token = [lemmatizer.lemmatize(word) for word in sent_token]\n    # Stemming\n#     sent_token = [ps.stem(word) for word in sent_token]\n    return sent_token","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.388604Z","iopub.execute_input":"2023-05-09T04:32:51.390234Z","iopub.status.idle":"2023-05-09T04:32:51.402769Z","shell.execute_reply.started":"2023-05-09T04:32:51.390111Z","shell.execute_reply":"2023-05-09T04:32:51.401244Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# train_df_en['sent1'] = train_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# train_df_en['sent2'] = train_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntrain_df_es['sent1'] = train_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntrain_df_es['sent2'] = train_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\n# val_df_en['sent1'] = val_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# val_df_en['sent2'] = val_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\nval_df_es['sent1'] = val_df_es['sent1'].apply(lambda x: preprocess_text(x))\nval_df_es['sent2'] = val_df_es['sent2'].apply(lambda x: preprocess_text(x))\n\n# test_df_en['sent1'] = test_df_en['sent1'].apply(lambda x: preprocess_text(x))\n# test_df_en['sent2'] = test_df_en['sent2'].apply(lambda x: preprocess_text(x))\n\ntest_df_es['sent1'] = test_df_es['sent1'].apply(lambda x: preprocess_text(x))\ntest_df_es['sent2'] = test_df_es['sent2'].apply(lambda x: preprocess_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.404337Z","iopub.execute_input":"2023-05-09T04:32:51.405299Z","iopub.status.idle":"2023-05-09T04:32:51.570712Z","shell.execute_reply.started":"2023-05-09T04:32:51.405233Z","shell.execute_reply":"2023-05-09T04:32:51.569429Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"total_sent_en = list(train_df_en['sent1']) + list(train_df_en['sent2'])\ntotal_sent_es= list(train_df_es['sent1']) + list(train_df_es['sent2'])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.572179Z","iopub.execute_input":"2023-05-09T04:32:51.572582Z","iopub.status.idle":"2023-05-09T04:32:51.582094Z","shell.execute_reply.started":"2023-05-09T04:32:51.572544Z","shell.execute_reply":"2023-05-09T04:32:51.580869Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"word_dict_en = {}\nfor word_tokens in total_sent_en:\n    for word in word_tokens:\n        if word in word_dict_en:\n            word_dict_en[word] += 1\n        else:\n            word_dict_en[word] = 1\n            \nvocab_length_en = len(word_dict_en)\n\nword_dict_es = {}\nfor word_tokens in total_sent_es:\n    for word in word_tokens:\n        if word in word_dict_es:\n            word_dict_es[word] += 1\n        else:\n            word_dict_es[word] = 1\n            \nvocab_length_es = len(word_dict_es)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.583804Z","iopub.execute_input":"2023-05-09T04:32:51.584356Z","iopub.status.idle":"2023-05-09T04:32:51.877680Z","shell.execute_reply.started":"2023-05-09T04:32:51.584281Z","shell.execute_reply":"2023-05-09T04:32:51.876640Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_sorted_counter(word_counter):\n    return {k: v for k, v in sorted(word_counter.items(), key=lambda item: item[1], reverse=False)}","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.879619Z","iopub.execute_input":"2023-05-09T04:32:51.880094Z","iopub.status.idle":"2023-05-09T04:32:51.886845Z","shell.execute_reply.started":"2023-05-09T04:32:51.880046Z","shell.execute_reply":"2023-05-09T04:32:51.885537Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sorted_counter_en = get_sorted_counter(word_dict_en)\nsorted_counter_es = get_sorted_counter(word_dict_es)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.888225Z","iopub.execute_input":"2023-05-09T04:32:51.888877Z","iopub.status.idle":"2023-05-09T04:32:51.909071Z","shell.execute_reply.started":"2023-05-09T04:32:51.888837Z","shell.execute_reply":"2023-05-09T04:32:51.907685Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_df_en['sent1'] = train_df_en['sent1'].astype(str).apply(eval)\ntrain_df_en['sent2'] = train_df_en['sent2'].astype(str).apply(eval)\n\ntrain_df_es['sent1'] = train_df_es['sent1'].astype(str).apply(eval)\ntrain_df_es['sent2'] = train_df_es['sent2'].astype(str).apply(eval)\n\nval_df_en['sent1'] = val_df_en['sent1'].astype(str).apply(eval)\nval_df_en['sent2'] = val_df_en['sent2'].astype(str).apply(eval)\n\nval_df_es['sent1'] = val_df_es['sent1'].astype(str).apply(eval)\nval_df_es['sent2'] = val_df_es['sent2'].astype(str).apply(eval)\n\ntest_df_en['sent1'] = test_df_en['sent1'].astype(str).apply(eval)\ntest_df_en['sent2'] = test_df_en['sent2'].astype(str).apply(eval)\n\ntest_df_es['sent1'] = test_df_es['sent1'].astype(str).apply(eval)\ntest_df_es['sent2'] = test_df_es['sent2'].astype(str).apply(eval)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:51.910929Z","iopub.execute_input":"2023-05-09T04:32:51.911336Z","iopub.status.idle":"2023-05-09T04:32:52.804874Z","shell.execute_reply.started":"2023-05-09T04:32:51.911301Z","shell.execute_reply":"2023-05-09T04:32:52.803886Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent2\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = train_df_en[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = train_df_es[\"sent1\"] \n\nscore = train_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_train = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.811638Z","iopub.execute_input":"2023-05-09T04:32:52.812331Z","iopub.status.idle":"2023-05-09T04:32:52.828136Z","shell.execute_reply.started":"2023-05-09T04:32:52.812288Z","shell.execute_reply":"2023-05-09T04:32:52.826748Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent2\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = val_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = val_df_es[\"sent1\"] \n\nscore = val_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_val = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.830789Z","iopub.execute_input":"2023-05-09T04:32:52.831157Z","iopub.status.idle":"2023-05-09T04:32:52.844973Z","shell.execute_reply.started":"2023-05-09T04:32:52.831124Z","shell.execute_reply":"2023-05-09T04:32:52.843967Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# assuming train_df_en and train_df_es are already defined and loaded with data\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_en[\"sent1\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent2\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_one = pd.concat([sent1_en, sent2_es, score], axis=1)\n\n\n# selecting the \"sent1\" column from train_df_en\nsent1_en = test_df_es[\"sent2\"] \n\n# selecting the \"sent2\" column from train_df_es\nsent2_es = test_df_es[\"sent1\"] \n\nscore = test_df_en[\"score\"] \n\n# concatenating the selected columns along with the \"score\" column\nnew_df_two = pd.concat([sent1_en, sent2_es, score], axis=1)\n\nfinal_test = pd.concat([new_df_one, new_df_two], ignore_index=True)\n# printing the new dataframe\n# final_train.len()","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.846386Z","iopub.execute_input":"2023-05-09T04:32:52.846799Z","iopub.status.idle":"2023-05-09T04:32:52.860459Z","shell.execute_reply.started":"2023-05-09T04:32:52.846763Z","shell.execute_reply":"2023-05-09T04:32:52.859398Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_sentences1 = list(final_train['sent1'])\ntrain_sentences2 = list(final_train['sent2'])\ntrain_similarity_scores = list(final_train['score'])\n\nval_sentences1 = list(final_val['sent1'])\nval_sentences2 = list(final_val['sent2'])\nval_similarity_scores = list(final_val['score'])\n\ntest_sentences1 = list(final_test['sent1'])\ntest_sentences2 = list(final_test['sent2'])\ntest_similarity_scores = list(final_test['score'])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.862066Z","iopub.execute_input":"2023-05-09T04:32:52.863154Z","iopub.status.idle":"2023-05-09T04:32:52.881006Z","shell.execute_reply.started":"2023-05-09T04:32:52.863114Z","shell.execute_reply":"2023-05-09T04:32:52.880027Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(len(train_sentences1))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.882317Z","iopub.execute_input":"2023-05-09T04:32:52.882977Z","iopub.status.idle":"2023-05-09T04:32:52.894233Z","shell.execute_reply.started":"2023-05-09T04:32:52.882931Z","shell.execute_reply":"2023-05-09T04:32:52.892817Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"11498\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_sentence_embedding_en(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_en_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [ft_en_vectors.get_word_vector(word) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding\n    \ndef get_sentence_embedding_es(sentence, max_length=30):\n        words = sentence\n        # filter out words that are not present in the model's vocabulary\n        words = [word for word in words if word in ft_es_vectors.words ]\n        # generate word embeddings for each word\n        embeddings = [np.matmul(ft_es_vectors.get_word_vector(word), transform.T) for word in words]\n        #mean strategy\n        embedding = np.mean(embeddings, axis=0)\n        return embedding","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.895783Z","iopub.execute_input":"2023-05-09T04:32:52.896348Z","iopub.status.idle":"2023-05-09T04:32:52.905351Z","shell.execute_reply.started":"2023-05-09T04:32:52.896314Z","shell.execute_reply":"2023-05-09T04:32:52.904068Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# print( val_sentences2)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.907171Z","iopub.execute_input":"2023-05-09T04:32:52.907547Z","iopub.status.idle":"2023-05-09T04:32:52.920653Z","shell.execute_reply.started":"2023-05-09T04:32:52.907512Z","shell.execute_reply":"2023-05-09T04:32:52.919718Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# # generate sentence embeddings\n# train_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in train_sentences1])\n# print(\"1\")\n# train_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in train_sentences2])\n# print(\"1\")\n\n# val_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in val_sentences1])\n# print(\"1\")\n# val_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in val_sentences2])\n# print(\"1\")\n\n\n# test_X1 = np.array([get_sentence_embedding_en(sentence) for sentence in test_sentences1])\n# print(\"1\")\n\n# test_X2 = np.array([get_sentence_embedding_es(sentence) for sentence in test_sentences2])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.921787Z","iopub.execute_input":"2023-05-09T04:32:52.922813Z","iopub.status.idle":"2023-05-09T04:32:52.932196Z","shell.execute_reply.started":"2023-05-09T04:32:52.922762Z","shell.execute_reply":"2023-05-09T04:32:52.931320Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# print(train_X2)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.933526Z","iopub.execute_input":"2023-05-09T04:32:52.934740Z","iopub.status.idle":"2023-05-09T04:32:52.944253Z","shell.execute_reply.started":"2023-05-09T04:32:52.934700Z","shell.execute_reply":"2023-05-09T04:32:52.943078Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# train_X = np.concatenate([train_X1, train_X2], axis=1)\n# val_X = np.concatenate([val_X1, val_X2], axis=1)\n# test_X = np.concatenate([test_X1, test_X2], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.945865Z","iopub.execute_input":"2023-05-09T04:32:52.946348Z","iopub.status.idle":"2023-05-09T04:32:52.956088Z","shell.execute_reply.started":"2023-05-09T04:32:52.946293Z","shell.execute_reply":"2023-05-09T04:32:52.954673Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def sts_score(sim_score):\n    sts_score = (sim_score+1) * 2.5\n    return sts_score","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.957634Z","iopub.execute_input":"2023-05-09T04:32:52.957973Z","iopub.status.idle":"2023-05-09T04:32:52.969655Z","shell.execute_reply.started":"2023-05-09T04:32:52.957941Z","shell.execute_reply":"2023-05-09T04:32:52.968234Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\nfrom scipy.stats import pearsonr\n\ndef get_sts_scores(emb1_lt, emb2_lt):\n    y_pred = []\n    for i in range(len(emb1_lt)):\n        sim_score = 1 - spatial.distance.cosine(emb1_lt[i], emb2_lt[i])\n        y_pred.append(sts_score(sim_score))\n    return y_pred\n    \ndef pearson_corr(y_true, y_pred):\n    \"\"\"\n    Calculate Pearson correlation coefficient between two arrays.\n    \"\"\"\n    corr, _ = pearsonr(y_true, y_pred)\n    return corr","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.970709Z","iopub.execute_input":"2023-05-09T04:32:52.971058Z","iopub.status.idle":"2023-05-09T04:32:52.982184Z","shell.execute_reply.started":"2023-05-09T04:32:52.971025Z","shell.execute_reply":"2023-05-09T04:32:52.980814Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Normalised Cosine Similarity","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport fasttext.util\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n# Load FastText embeddings\nft_es_vectors = fasttext.load_model('/kaggle/input/fasttext-spanish-300/cc.es.300.bin')\nft_en_vectors = fasttext.load_model('/kaggle/input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin')\n\n# Get the word vectors and corresponding words for both languages\nwords_es = ft_es_vectors.get_words()\nvectors_es = np.array([ft_es_vectors.get_word_vector(word) for word in words_es])\n\nwords_en = ft_en_vectors.get_words()\nvectors_en = np.array([ft_en_vectors.get_word_vector(word) for word in words_en])\n\n# Convert the word vectors to PyTorch tensors\nvectors_es_tensor = torch.from_numpy(vectors_es).float()\nvectors_en_tensor = torch.from_numpy(vectors_en).float()\n\n# Define a simple feed-forward neural network for mapping\nclass MappingNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(MappingNetwork, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.fc(x)\n\n# Create the mapping network\ninput_dim = vectors_es_tensor.shape[1]\noutput_dim = vectors_en_tensor.shape[1]\nmapping_network = MappingNetwork(input_dim, output_dim)\n\n# Define the loss function and optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(mapping_network.parameters(), lr=0.001)\n\n# Train the mapping network\ndataset = DataLoader(list(zip(vectors_es_tensor, vectors_en_tensor)), batch_size=1000, shuffle=True)\n\nnum_epochs = 10\ntotal_steps = len(dataset) * num_epochs\n\nfor epoch in range(num_epochs):\n    for step, batch in enumerate(dataset):\n        src_vectors, tgt_vectors = batch\n        optimizer.zero_grad()\n        mapped_vectors = mapping_network(src_vectors)\n        loss = loss_fn(mapped_vectors, tgt_vectors)\n        loss.backward()\n        optimizer.step()\n\n        # Calculate progress in percentage\n        progress = (epoch * len(dataset) + step + 1) / total_steps * 100\n\n        # Print epoch and progress\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(dataset)}], Progress: {progress:.2f}%\")\n\n# Map the source language word vectors to the target language space\nmapped_vectors_es = mapping_network(vectors_es_tensor).detach().numpy()\n\n# Calculate cosine similarity between mapped source vectors and target vectors\ncos_sim_scores = np.dot(mapped_vectors_es, vectors_en.T) / (\n        np.linalg.norm(mapped_vectors_es, axis=1).reshape(-1, 1) * np.linalg.norm(vectors_en, axis=1))\n\n# \n\nimport numpy as np\n\n# Example English and Spanish sentences\nsentence_en = \"I like cats\"\nsentence_es = \"Me gustan los gatos\"\n\n# Tokenize the sentences into words\ntokens_en = sentence_en.lower().split()\ntokens_es = sentence_es.lower().split()\n\n# Calculate sentence embeddings by averaging word vectors\nembedding_en = np.mean([mapped_vectors_es[words_es.index(token)] for token in tokens_en], axis=0)\nembedding_es = np.mean([vectors_es[words_es.index(token)] for token in tokens_es], axis=0)\n\n# Calculate cosine similarity between sentence embeddings\ncos_sim = np.dot(embedding_en, embedding_es) / (np.linalg.norm(embedding_en) * np.linalg.norm(embedding_es))\n\nprint(\"Cosine Similarity:\", cos_sim)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-09T04:32:52.983982Z","iopub.execute_input":"2023-05-09T04:32:52.984344Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\nWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Step [2000/2000], Progress: 10.00%\nEpoch [2/10], Step [2000/2000], Progress: 20.00%\nEpoch [3/10], Step [2000/2000], Progress: 30.00%\nEpoch [4/10], Step [2000/2000], Progress: 40.00%\nEpoch [5/10], Step [2000/2000], Progress: 50.00%\nEpoch [6/10], Step [2000/2000], Progress: 60.00%\nEpoch [7/10], Step [2000/2000], Progress: 70.00%\nEpoch [8/10], Step [2000/2000], Progress: 80.00%\nEpoch [9/10], Step [2000/2000], Progress: 90.00%\nEpoch [10/10], Step [2000/2000], Progress: 100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Example English and Spanish sentences\nsentence_en = \"I like cats\"\nsentence_es = \"Me gustan los gatos\"\n\n# Tokenize the sentences into words\ntokens_en = sentence_en.lower().split()\ntokens_es = sentence_es.lower().split()\n\n# Calculate sentence embeddings by averaging word vectors\nembedding_en = np.mean([mapped_vectors_es[words_es.index(token)] for token in tokens_en], axis=0)\nembedding_es = np.mean([vectors_es[words_es.index(token)] for token in tokens_es], axis=0)\n\n# Calculate cosine similarity between sentence embeddings\ncos_sim = np.dot(embedding_en, embedding_es) / (np.linalg.norm(embedding_en) * np.linalg.norm(embedding_es))\n\nprint(\"Cosine Similarity:\", cos_sim)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of computing similarity between two words\nword_idx_es = list(words_es).index('manzana')\nword_idx_en = list(words_en).index('apple')\nsimilarity_score = cos_sim_scores[word_idx_es][word_idx_en]\nprint(\"Similarity score between 'manzana' and 'apple':\", similarity_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y_pred = get_sts_scores(train_X1, train_X2)\nval_y_pred = get_sts_scores(val_X1, val_X2)\ntest_y_pred = get_sts_scores(test_X1, test_X2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(train_similarity_scores, train_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(val_similarity_scores, val_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pearson_corr(test_similarity_scores, test_y_pred)\nprint(\"Pearson correlation coefficient: {:.2f}\".format(corr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Residual Analysis","metadata":{}},{"cell_type":"code","source":"row_max_len = lambda row: max(len(row['sent1']), len(row['sent2']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['sent_len'] = train_df.apply(row_max_len, axis=1)\nval_df['sent_len'] = val_df.apply(row_max_len, axis=1)\ntest_df['sent_len'] = test_df.apply(row_max_len, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['diff'] = abs(train_df['score'] - train_y_pred)\nval_df['diff'] = abs(val_df['score'] - val_y_pred)\ntest_df['diff'] = abs(test_df['score'] - test_y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_df = train_df.groupby(['sent_len']).mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_df['sent_length'] = grouped_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(list(grouped_df.index), list(grouped_df['diff']))\nplt.title(\"average error trend in training data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"average difference\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_val = val_df.groupby(['sent_len']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(list(grouped_val.index), list(grouped_val['diff']))\nplt.title(\"average error trend in validation data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"average difference\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_test = test_df.groupby(['sent_len']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(list(grouped_test.index), list(grouped_test['diff']))\nplt.title(\"average error trend in test data\")\nplt.xlabel(\"sentence length\")\nplt.ylabel(\"average difference\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}